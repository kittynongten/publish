{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list =  [input('\\nข้อความ : ')] # รับ input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['']\n"
     ]
    }
   ],
   "source": [
    "print(text_list)\n",
    "#print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def clean_msg(msg):\n",
    "    \n",
    "    \n",
    "    # ลบ text ที่อยู่ในวงเล็บ <> ทั้งหมด\n",
    "    msg = re.sub(r'<.*?>','', msg)\n",
    "    \n",
    "    # ลบ hashtag\n",
    "    msg = re.sub(r'#','',msg)\n",
    "    \n",
    "    # ลบ …\n",
    "    msg = re.sub(r'…','',msg)\n",
    "    \n",
    "    # ลบ เครื่องหมายคำพูด (punctuation)\n",
    "    for c in string.punctuation:\n",
    "        msg = re.sub(r'\\{}'.format(c),'',msg)\n",
    "    \n",
    "    # ลบ separator เช่น \\n \\t\n",
    "    msg = ' '.join(msg.split())\n",
    "    \n",
    "    return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "original text:\n \nclean text:\n \n"
     ]
    }
   ],
   "source": [
    "print('original text:\\n',text_list[0])\n",
    "print('clean text:\\n',clean_msg(text_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "list index out of range",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-e17e839a7326>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'original text:\\n'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtext_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'clean text:\\n'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclean_msg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print('original text:\\n',text_list[1])\n",
    "print('clean text:\\n',clean_msg(text_list[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text = [clean_msg(txt) for txt in text_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pythainlp\n",
    "#!pip install stop_words\n",
    "import pythainlp\n",
    "from pythainlp import word_tokenize\n",
    "from pythainlp.corpus import thai_stopwords\n",
    "from pythainlp.corpus import wordnet\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import words\n",
    "from stop_words import get_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package words to\n[nltk_data]     C:\\Users\\Seena\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('words')\n",
    "th_stop = tuple(thai_stopwords())\n",
    "en_stop = tuple(get_stop_words('en'))\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "#print(th_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_word(text):\n",
    "            \n",
    "    \n",
    "    tokens = word_tokenize(text,engine='newmm')\n",
    "    \n",
    "    # Remove stop words ภาษาไทย และภาษาอังกฤษ\n",
    "    tokens = [i for i in tokens if not i in th_stop and not i in en_stop]\n",
    "    #tokens = [i for i in tokens ]\n",
    "    \n",
    "    # หารากศัพท์ภาษาไทย และภาษาอังกฤษ\n",
    "    # English\n",
    "    tokens = [p_stemmer.stem(i) for i in tokens]\n",
    "    \n",
    "    # Thai\n",
    "    tokens_temp=[]\n",
    "    for i in tokens:\n",
    "        w_syn = wordnet.synsets(i)\n",
    "        if (len(w_syn)>0) and (len(w_syn[0].lemma_names('tha'))>0):\n",
    "            tokens_temp.append(w_syn[0].lemma_names('tha')[0])\n",
    "        else:\n",
    "            tokens_temp.append(i)\n",
    "    \n",
    "    tokens = tokens_temp\n",
    "    \n",
    "    # ลบตัวเลข\n",
    "    tokens = [i for i in tokens if not i.isnumeric()]\n",
    "    \n",
    "    # ลบช่องว่าง\n",
    "    tokens = [i for i in tokens if not ' ' in i]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tokenized text:\n ['สวัสดี']\n"
     ]
    }
   ],
   "source": [
    "print('tokenized text:\\n',split_word(clean_msg(text_list[0])))\n",
    "#print('tokenized text:\\n',split_word(clean_msg(text_list[1])))\n",
    "#print('tokenized text:\\n',split_word(clean_msg(text_list[2])))\n",
    "#print('tokenized text:\\n',split_word(clean_msg(text_list[3])))\n",
    "#print('tokenized text:\\n',split_word(clean_msg(text_list[4])))\n",
    "#print('tokenized text:\\n',split_word(clean_msg(text_list[5])))\n",
    "#print('tokenized text:\\n',split_word(clean_msg(text_list[6])))\n",
    "#print('tokenized text:\\n',split_word(clean_msg(text_list[7])))\n",
    "#print('tokenized text:\\n',split_word(clean_msg(text_list[8])))\n",
    "#print('tokenized text:\\n',split_word(clean_msg(text_list[9])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_list = [split_word(txt) for txt in clean_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['สวัสดี']]"
      ]
     },
     "metadata": {},
     "execution_count": 149
    }
   ],
   "source": [
    "tokens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "tokens_list_j = [','.join(tkn) for tkn in tokens_list]\n",
    "cvec = CountVectorizer(analyzer=lambda x:x.split(','))\n",
    "c_feat = cvec.fit_transform(tokens_list_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['สวัสดี']\n"
     ]
    }
   ],
   "source": [
    "vocab = cvec.vocabulary_\n",
    "print(tokens_list_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "matrix([[1]], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 152
    }
   ],
   "source": [
    "c_feat[:,:20].todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tvec = TfidfVectorizer(analyzer=lambda x:x.split(','),)\n",
    "t_feat = tvec.fit_transform(tokens_list_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "matrix([[1.]])"
      ]
     },
     "metadata": {},
     "execution_count": 154
    }
   ],
   "source": [
    "t_feat[:,:5].todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 1\n"
     ]
    }
   ],
   "source": [
    "print(len(tvec.idf_),len(tvec.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "matrix([[1]], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 156
    }
   ],
   "source": [
    "c_feat[:,:5].todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[1]]\n"
     ]
    }
   ],
   "source": [
    "print(c_feat.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(c_feat.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1]], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 160
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    " y = np.array(['+', '+', '-', '-','+','-','+','+','-','+'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'MultinomialNB' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8aaea68955e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_prior\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Train model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MultinomialNB' is not defined"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB(class_prior=[0.25, 0.5])\n",
    "\n",
    "# Train model\n",
    "model = clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MultinomialNB(class_prior=[0.25, 0.5])\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new observation\n",
    "new_observation = [[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict new observation's class\n",
    "y_predicted = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data to disk\n",
    "data = 'y_predicted.pkl'\n",
    "pickle.dump(y_predicted, open(data, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-'\n '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-'\n '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-'\n '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-'\n '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-'\n '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-'\n '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-'\n '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-'\n '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-'\n '-' '-' '-' '-' '-' '-' '-' '+' '+' '+' '+' '+' '+' '+' '+' '+' '+' '+'\n '+' '+' '+' '+' '+' '+' '+' '+' '+' '+' '+' '+' '+' '+' '+' '+' '+' '+'\n '+' '+' '+' '+' '-' '+' '+' '+' '+' '+' '+' '+' '+' '+' '+' '+' '+' '+'\n '+' '-' '-' '+' '+' '+' '+' '+' '+' '+' '+' '+' '+' '-' '+' '+' '+' '+'\n '+' '+' '+' '+' '+' '+' '+' '-' '-' '+' '+' '+' '+' '+' '+' '-' '+' '-'\n '+' '-' '+' '+' '+' '-' '+' '-' '+' '+' '+' '-' '+' '-' '+' '-' '-' '+'\n '+' '+' '-' '+' '+' '+' '+' '+' '+' '-' '+' '+' '+' '-' '+' '+' '-' '+'\n '+' '-' '+' '+' '+' '+' '-' '+' '+' '+' '+' '+' '-' '-' '+' '-' '-' '+'\n '+' '+' '+' '+' '+' '-' '+' '+' '-' '+' '-' '+' '-' '-' '-' '+' '-' '+'\n '+' '-' '+' '+' '+' '+' '-' '+' '-' '+' '+' '+']\n"
     ]
    }
   ],
   "source": [
    "print(y_predicted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "4a3b8d4a28362c21555e1054d936602574c03f5935c3d8507dffaa63ab76ed18"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}